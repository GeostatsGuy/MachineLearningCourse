{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31157ec3",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "## Subsurface Data Analytics Final Project\n",
    "\n",
    "### Impact of Outliers on Margin Length of Support Vector Machines\n",
    "\n",
    "#### Barun Das, Graduate Student, The University of Texas at Austin\n",
    "\n",
    "#### Lei Liu, Graduate Student, The University of Texas at Austin\n",
    "\n",
    "#### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "\n",
    "### PGE 383 Final Project: Support Vector Machines (SVM) for Subsurface Modeling in Python \n",
    "\n",
    "#### Table of Contents\n",
    "#### 1) Executive Summary\n",
    "#### 2) SVM Introduction\n",
    "#### 3) Mathematical Concepts behind SVMs\n",
    "#### 4) Applications of SVMs\n",
    "\n",
    "#### 1) Executive Summary\n",
    "\n",
    "The following project shows the impact of changing the number outliers on the margin size of SVMs. The SVC functionality of scikit-learn is employed to generate the SVMs. It is shown that as the number of outliers increase, the margins also increase. \n",
    "\n",
    "#### 2) SVM Introduction\n",
    "\n",
    "Support Vector Machines (SVMs) are powerful supervised learning models used for classification and regression tasks. SVMs excel in finding the optimal decision boundary, known as the hyperplane, that best separates different classes in the input space. They work by identifying a hyperplane that maximizes the margin, the distance between the hyperplane and the closest data points of each class, hence aiming for better generalization to new, unseen data. SVMs are effective in high-dimensional spaces, even when the number of dimensions exceeds the number of samples, making them suitable for various machine learning tasks, including both linear and non-linear classification through the use of different kernel functions like polynomial, radial basis function (RBF), or sigmoid functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b7c1bd",
   "metadata": {},
   "source": [
    "#### Mathematical Concepts behind SVMs\n",
    "\n",
    "**1. Hyperplane:**\n",
    "SVMs aim to find the optimal hyperplane that separates classes in the input space. For a binary classification problem, the hyperplane is represented by the equation $$ w \\cdot x + b = 0 $$ in a feature space, where  $w$ is the weight vector perpendicular to the hyperplane, $x$ represents the input features, and $ b $ is the bias term.\n",
    "\n",
    "**2. Margin:**\n",
    "The margin is the distance between the hyperplane and the nearest data points of each class, known as support vectors. SVMs aim to maximize this margin. Mathematically, the margin is given by $\\frac{2}{\\|w\\|}$, where $\\|w\\|$ represents the Euclidean norm of the weight vector.\n",
    "\n",
    "**3. Objective Function:**\n",
    "SVMs use an objective function to optimize the hyperplane. The objective is to maximize the margin while minimizing classification errors. This is typically formulated as minimizing $ \\frac{1}{2}\\|w\\|^2 $ subject to the constraints that for each data point $(x_i, y_i)$, where $x_i$ is the input and $y_i$ is the class label (-1 or 1), $y_i(w \\cdot x_i + b) \\geq 1 $ for points lying on or inside the margin.\n",
    "\n",
    "**4. Lagrange Multipliers and Dual Formulation:**\n",
    "Solving the optimization problem involves using Lagrange multipliers to convert it into its dual form, allowing for more efficient computation. This leads to expressing the problem in terms of Lagrange multipliers $\\alpha_i$ and formulating the dual objective function that needs to be maximized, which involves summations over pairs of data points.\n",
    "\n",
    "**5. Kernel Trick:**\n",
    "For non-linearly separable data, SVMs use the kernel trick to implicitly map the input data into a higher-dimensional space where the classes might be linearly separable. This allows the SVM to find a linear decision boundary in the higher-dimensional space without explicitly computing the transformed features.\n",
    "\n",
    "**6. Optimization:**\n",
    "The optimization process involves solving the dual objective function using optimization techniques like Sequential Minimal Optimization (SMO) or quadratic programming to find the optimal $\\alpha_i$ values, which then allow computation of $w$ and $b$ to define the separating hyperplane.\n",
    "\n",
    "Overall, SVMs offer an effective way to find the best possible separation between classes by maximizing the margin and handling both linear and non-linear classification tasks by using appropriate kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ae110",
   "metadata": {},
   "source": [
    "#### Applications SVMs\n",
    "\n",
    "Support Vector Machines (SVMs) find applications across various domains due to their effectiveness in both classification and regression tasks. Some typical uses of SVMs include:\n",
    "\n",
    "#### 1. **Image Classification**\n",
    "   SVMs are used in image classification tasks, such as object recognition, facial expression analysis, and handwritten digit recognition. They perform well in distinguishing between different classes within images.\n",
    "\n",
    "#### 2. **Text and Document Classification**\n",
    "   In Natural Language Processing (NLP), SVMs are employed for text classification tasks like sentiment analysis, spam detection, topic classification, and document categorization.\n",
    "\n",
    "#### 3. **Biomedical Applications**\n",
    "   SVMs aid in medical diagnosis, such as cancer classification from tissue samples, identifying disease risk factors from genetic data, and predicting patient outcomes based on medical records.\n",
    "\n",
    "#### 4. **Financial Forecasting**\n",
    "   SVMs are used in stock market forecasting, credit scoring, fraud detection, and risk assessment due to their ability to handle complex data and make accurate predictions.\n",
    "\n",
    "#### 5. **Handwriting Recognition**\n",
    "   They are utilized in Optical Character Recognition (OCR) systems to recognize handwritten characters or text in forms, documents, or bank checks.\n",
    "\n",
    "#### 6. **Bioinformatics**\n",
    "   SVMs play a role in genomics, proteomics, and other bioinformatics fields for tasks like protein classification, gene expression analysis, and protein structure prediction.\n",
    "\n",
    "#### 7. **Remote Sensing**\n",
    "   SVMs analyze remote sensing data for land cover classification, crop yield prediction, object detection in satellite images, and environmental monitoring.\n",
    "\n",
    "#### 8. **Anomaly Detection**\n",
    "   SVMs are effective in anomaly detection tasks, such as detecting intrusions in network security, identifying defective products in manufacturing, or finding outliers in datasets.\n",
    "\n",
    "#### 9. **Regression Analysis**\n",
    "   Though primarily a classification algorithm, SVMs can also be used for regression tasks by modifying the formulation to predict continuous values.\n",
    "\n",
    "#### 10. **Feature Extraction and Dimensionality Reduction**\n",
    "   SVMs contribute to feature selection and dimensionality reduction techniques, assisting in reducing the complexity of high-dimensional data while preserving important information.\n",
    "\n",
    "SVMs are versatile and perform well in various scenarios, especially when the data is well-structured and there's a need for a clear separation between classes or groups in the dataset. However, their performance might be affected by large datasets or noise, and careful selection of hyperparameters is crucial for optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e65dd6",
   "metadata": {},
   "source": [
    "#### Step 1: Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2583c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from ipywidgets import interact, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e379af",
   "metadata": {},
   "source": [
    "#### Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef26ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "y = np.where(y == 0, -1, 1)  # Change labels to -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af419465",
   "metadata": {},
   "source": [
    "#### Create plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b0ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(num_outliers):\n",
    "    np.random.seed(42)\n",
    "    outliers_indices = np.random.choice(len(X), num_outliers, replace=False)\n",
    "    y_copy = y.copy()\n",
    "    y_copy[outliers_indices] *= -1  # Flip the labels for outliers\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_copy, cmap='viridis')\n",
    "\n",
    "    # Fit SVM with outliers\n",
    "    svm = SVC(kernel='linear', C=1.0)\n",
    "    svm.fit(X, y_copy)\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = svm.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # Plot decision boundary and margins\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k')\n",
    "    plt.title(f\"SVM with {num_outliers} Outliers\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314494c",
   "metadata": {},
   "source": [
    "#### Create slider for number of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410d20b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30250bd5318421687407bf73139b2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Num Outliers:', max=20), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_svm(num_outliers)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(plot_svm, num_outliers=IntSlider(min=0, max=20, step=1, value=0, description='Num Outliers:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07762ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
